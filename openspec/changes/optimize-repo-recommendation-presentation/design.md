# Design: Optimize Repository Recommendation Presentation

## Context
Current system presents 5 repositories ranked by ACS score without explaining trade-offs. Users struggle with:
1. **Decision paralysis**: 5 similar-looking options without clear differentiation
2. **Lack of guidance**: Why choose RepoA over RepoB? Not obvious from scores alone
3. **Missed opportunity**: ACS dimensions (Interface Clarity, Documentation, Environment, Token Economy) contain rich information about trade-offs but aren't surfaced

This change leverages existing ACS scoring to provide **personalized guidance** that mimics a human consultant's reasoning.

## Goals
- **Primary**: Reduce user decision time by 50% through clearer recommendation hierarchy
- **Secondary**: Help users understand trade-offs between alternatives
- **Tertiary**: Increase conversion rate (user chooses and converts a repo to skill faster)

## Non-Goals
- Machine learning / preference learning (use static LLM reasoning for now)
- A/B testing framework (leave instrumentation for future)
- User profile persistence (stateless recommendations)

## Decisions

### Decision 1: Where to Generate Reasoning
**Question**: Should reasoning be generated by:
- (A) LLM in Consultant Agent (at tool result time)
- (B) Frontend logic (at display time)
- (C) H2 Discovery pipeline (at scoring time)

**Decision**: **(A) LLM in Consultant Agent**

**Rationale**:
- LLM has full context of user's query + conversation history
- Can explain trade-offs in natural language tailored to user's needs
- Consultant Agent is already the "translator" between discovery results and user
- Avoids duplicating NLP in frontend
- Simple to implement (just enhance system prompt)

**Alternatives**:
- (B) Frontend logic: Would require hardcoding rules about ACS dimensions; not scalable
- (C) H2 Discovery: Adds latency to scoring phase; would need to run again per query

### Decision 2: Recommendation Structure
**Question**: How to structure the recommendation output?

**Decision**: Use **top-1 + alternatives** model (not star ratings or confidence scores)

**Rationale**:
- Simpler mental model: "This is the best, but if you need X, try this instead"
- Forces clear differentiation (not "also good")
- Easier to explain to users
- Aligns with UX best practices for recommendation UI

**Alternatives**:
- Multi-criteria ranking (complex, requires user preferences)
- Confidence scores (may confuse with ACS score)

### Decision 3: When to Show Alternatives
**Question**: Always show all 5 alternatives, or be selective?

**Decision**: **Progressive disclosure**
- Show top recommendation prominently
- Show alternatives in collapsible/secondary section by default
- Always show alternatives if score gap < 10 points (close competition)
- Don't show alternatives if score gap > 15 points (clear winner)

**Rationale**:
- Reduces cognitive load on initial view
- Still allows exploration for curious users
- Reflects confidence levels (clear winner = fewer options)

**Alternatives**:
- Show all 5 always: Causes decision paralysis (original problem)
- Never show alternatives: Misses valid use cases

### Decision 4: Reasoning Format
**Question**: How to phrase "choose if" conditions?

**Decision**: **Reference specific ACS dimension or use case**

Examples:
- ✅ "Choose this if you need better documentation"
- ✅ "Choose this if you prefer CLI tools over libraries"
- ✅ "Choose this if token economy is your priority"
- ❌ "Also a good choice"
- ❌ "Another viable option"

**Rationale**:
- Specific reasons help users match repos to their constraints
- Educates users about what ACS dimensions mean
- Actionable (user can decide if this dimension matters to them)

### Decision 5: No New LLM Calls
**Question**: Should we add a separate "recommendation reasoning" LLM call?

**Decision**: **No, reuse existing Consultant Agent LLM context**

**Rationale**:
- Consultant already calls LLM to interpret results
- Minimal cost to add reasoning to existing call
- Avoids latency overhead
- Keeps conversation context intact

**Implementation**: Enhance system prompt to include instructions for analyzing ACS dimensions when formatting results.

## Architecture

```
User Query
    ↓
Consultant Agent (LLM)
    ├─ Calls findRepository tool
    │   ↓
    │ H2 Discovery Pipeline
    │   └─ Returns: top 5 scored repos + ACS breakdown
    │   ↓
    ├─ Receives results
    ├─ LLM analyzes ACS dimensions in context of query
    ├─ Generates reasoning:
    │   - Why top choice is best
    │   - When/why to choose each alternative
    └─ Formats response with structured data
         ↓
Frontend
    ├─ Displays top recommendation prominently
    ├─ Shows ACS breakdown chart
    ├─ Collapses alternatives in secondary section
    └─ Interactive comparison helper
```

## Risks & Mitigations

### Risk 1: LLM reasoning inconsistency
**Problem**: LLM might generate vague or unhelpful reasoning ("This is a good choice")

**Mitigation**:
- Provide explicit instructions in system prompt with examples
- Test with 10+ sample queries before deployment
- Include validation in testing (check reasoning is specific)

### Risk 2: UX confusion with old system prompt
**Problem**: Users/logs might reference old behavior

**Mitigation**: Clear changelog in release notes; brief migration message in UI if needed

### Risk 3: Reasoning doesn't match ACS scores
**Problem**: LLM reasoning contradicts ACS (e.g., recommends repo with low Documentation score for "best docs")

**Mitigation**:
- Include ACS scores in system prompt as context
- Validate test cases (reasoning should correlate with ACS)
- Consider adding "temperature: 0.5" for more consistency

### Risk 4: Frontend UX not intuitive
**Problem**: Users still confused about trade-offs despite better UI

**Mitigation**:
- Test with 3-5 users (usability testing)
- Iterate on "comparison helper" tooltip/popover
- Consider adding explanatory text: "What are these dimensions?"

## Trade-offs

| Aspect | Trade-off |
|--------|-----------|
| **Latency** | Adding reasoning analysis is minimal (<100ms LLM); acceptable |
| **Cost** | Slightly higher LLM tokens for reasoning generation; negligible at current scale |
| **Complexity** | Added logic to system prompt + output formatting; acceptable given UX benefit |
| **Personalization** | Static reasoning (no user preference learning); sufficient for MVP |

## Migration Plan

### Rollout Strategy
1. **Backward compatibility**: Keep existing tool output format; add new fields alongside
2. **Gradual rollout**: Deploy to 10% of users first
3. **Monitoring**: Track reasoning quality; rollback if LLM reasoning poor

### Rollback Plan
- If reasoning is unhelpful: Revert system prompt change + hide reasoning fields in UI
- If UX confuses users: Revert to old layout (full 5-repo list)
- **Rollback latency**: <5 min (just redeploy)

## Open Questions

1. **User Preferences**: Should we eventually learn user preferences (e.g., "I always want fastest setup")? Defer to Phase 2.
2. **Instrumentation**: Should we add analytics to measure decision time? Recommend: add later if needed.
3. **Scope Expansion**: Should "recommendation reasoning" apply to other recommendation types (Auditor results, etc.)? Out of scope for now.

## Success Metrics
- User chooses a recommendation faster (baseline: measure current conversion funnel)
- Fewer follow-up questions about "why this repo?" (inferred from conversation length)
- Higher conversion rate from results → fabrication (measure click-through)

## Implementation Notes

### System Prompt Enhancement
Current prompt focuses on **presenting results**.
New instruction should add:
```
"When presenting the top recommendation:
1. Explain which ACS dimension(s) make it ideal for this user's query
2. For alternatives, provide a specific reason to choose each one
3. Format alternatives as: 'Choose this if you need [specific benefit]'"
```

### Output Format
Current: Array of 5 repos with ACS scores
New: 
```json
{
  "topRecommendation": {
    "repo": {...},
    "reason": "This repo has the clearest CLI interface, perfect for your quick integration needs"
  },
  "alternatives": [
    {
      "repo": {...},
      "reason": "Choose this if you need better documentation and don't mind slower performance"
    }
  ]
}
```

### Frontend Component Updates
Scout block needs to shift from "list view" to "recommendation view":
- Primary card: top recommendation with reason
- Secondary section: alternatives with "choose if" conditions
- Interactive: hover/click on alternative to see comparison
